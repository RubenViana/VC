{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOsJ5IjcAVE-"
      },
      "source": [
        "## Lab 9 Part 1: Semantic Segmentation\n",
        "\n",
        "This notebook is about image segmentation.\n",
        "\n",
        "We will perform augmentations using ```albumentations``` from [https://github.com/albumentations-team/albumentations](https://github.com/albumentations-team/albumentations). The advantage of this package in comparison with using pytorch augmentations is that it enables applying the same augmentation to an image and its segmentation mask. We will also use ```torchmetrics``` to obtain evaluation metrics. First, we need to install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ppo-YL4tmx1",
        "outputId": "369e0cb3-1dcf-40c1-871d-649749253cda"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U albumentations\n",
        "!echo \"$(pip freeze | grep albumentations) is successfully installed\"\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soMygqHFr_-i"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchsummary import summary\n",
        "from torchmetrics import JaccardIndex\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNvgtC5RBEFQ"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "Let's start by downloading the data. We will be using the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIeJGWYJVSDR",
        "outputId": "1daf6956-bf31-4440-b7aa-72af78aef505"
      },
      "outputs": [],
      "source": [
        "!curl -L -O https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "!curl -L -O https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmxjYLUKglqh"
      },
      "source": [
        "The data was downloaded into the notebook. You can see it in Files (to the left of the notebook) in Google Colab. The images are in the folder ```images``` and the masks in ```annotations/trimaps```.\n",
        "\n",
        "In the annotations you will notice a file named ```trainval.txt``` which contains the names of images typically used for training and validation, and ```test.txt``` which contains the images typically used for testing. Let's separate the data into training, validation and testing based on these files.\n",
        "\n",
        "For today's class, we will only be using a part of the images during training (2000 for training, 500 for validation), to reduce the amount of time needed to run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKKzhOiIr3Y0"
      },
      "outputs": [],
      "source": [
        "images_directory = \"images\"\n",
        "masks_directory = os.path.join(\"annotations\", \"trimaps\")\n",
        "trainval_filename = os.path.join(\"annotations\", \"trainval.txt\")\n",
        "test_filename = os.path.join(\"annotations\", \"test.txt\")\n",
        "\n",
        "# Obtain names of images for training and validation\n",
        "trainval_file = pd.read_csv(trainval_filename, sep=\"\\s+\", header=None)\n",
        "trainval_images_filenames = np.asarray(trainval_file[0].values)\n",
        "trainval_images_filenames = np.asarray([x + \".jpg\" for x in trainval_images_filenames])\n",
        "\n",
        "# Shuffle images\n",
        "random.seed(42)\n",
        "random.shuffle(trainval_images_filenames)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "train_images_filenames = trainval_images_filenames[:2000]\n",
        "val_images_filenames = trainval_images_filenames[2000:2500]\n",
        "\n",
        "# Obtain names of images for testing\n",
        "test_file = pd.read_csv(test_filename, sep=\"\\s+\", header=None)\n",
        "test_images_filenames = np.asarray(test_file[0].values)\n",
        "test_images_filenames = np.asarray([x + \".jpg\" for x in test_images_filenames])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnrawAROglqi"
      },
      "source": [
        "This dataset has 3 labels, identified by integers from 1 to 3: pet (1), background (2), and border (3).\n",
        "\n",
        "We can convert it to a binary problem by merging regions corresponding to pets and borders, obtaining binary masks with 2 labels: background (0) and pet/border (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faXhXSbUs1T4"
      },
      "outputs": [],
      "source": [
        "# Convert original mask into a binary mask\n",
        "def preprocess_mask(mask):\n",
        "    mask = mask.astype(np.float32)\n",
        "    mask[mask == 2.0] = 0.0\n",
        "    mask[(mask == 1.0) | (mask == 3.0)] = 1.0\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3hQ2qB-glqi"
      },
      "source": [
        "Visualize examples from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X-1LzO8Vs451",
        "outputId": "d059b91a-bd0a-4a99-ab50-b9b1ff0fad46"
      },
      "outputs": [],
      "source": [
        "def display_image_grid(images_filenames, images_directory, masks_directory):\n",
        "    rows = len(images_filenames)\n",
        "    _, ax = plt.subplots(nrows=rows, ncols=2, figsize=(10, 24))\n",
        "    for i, image_filename in enumerate(images_filenames):\n",
        "        image = cv2.imread(os.path.join(images_directory, image_filename))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n",
        "        mask = preprocess_mask(mask)\n",
        "        ax[i, 0].imshow(image)\n",
        "        ax[i, 1].imshow(mask, interpolation=\"nearest\", cmap='gray')\n",
        "\n",
        "        ax[i, 0].set_title(\"Image\")\n",
        "        ax[i, 1].set_title(\"Ground truth mask\")\n",
        "\n",
        "        ax[i, 0].set_axis_off()\n",
        "        ax[i, 1].set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_image_grid(train_images_filenames[:5], images_directory, masks_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjHlP3YVglqi"
      },
      "source": [
        "## Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdoRdGNftYLX"
      },
      "outputs": [],
      "source": [
        "class OxfordPetDataset(Dataset):\n",
        "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
        "        self.images_filenames = images_filenames\n",
        "        self.images_directory = images_directory\n",
        "        self.masks_directory = masks_directory\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.images_filenames[idx]\n",
        "\n",
        "        # Read image\n",
        "        image = cv2.imread(os.path.join(self.images_directory, image_filename))\n",
        "\n",
        "        # Convert from BGR to RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Normalize image into 0-1 range\n",
        "        # Note that the ToTensorV2 method from the albumentations library does not automatically convert the image range into 0-1\n",
        "        image = image / 255.\n",
        "\n",
        "        # Read mask\n",
        "        mask = cv2.imread(\n",
        "            os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\")), cv2.IMREAD_UNCHANGED,\n",
        "        )\n",
        "\n",
        "        # Preprocess mask by converting it into a binary mask\n",
        "        mask = preprocess_mask(mask)\n",
        "\n",
        "        # Apply the same data augmentation to both input image and target mask\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "        return image.float(), mask.to(torch.int64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnvMGh9qglqj"
      },
      "source": [
        "Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJz6Z_UAudIw"
      },
      "outputs": [],
      "source": [
        "nr_classes = 2 # background vs foreground/pet\n",
        "batch_size = 4\n",
        "num_workers = 2\n",
        "epochs = 10\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Get device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34NxyAB_glqj"
      },
      "source": [
        "Define transformations to be applied to the images and data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE2SnTPquWuc"
      },
      "outputs": [],
      "source": [
        "# Define transformations/augmentations to be applied to training data\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomCrop(224, 224),\n",
        "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define dataloader for training data\n",
        "train_dataset = OxfordPetDataset(train_images_filenames, images_directory, masks_directory, transform=train_transform,)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "# Define transformations to be applied to validation data\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(256, 256),\n",
        "        A.CenterCrop(224, 224),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define dataloader for validation data\n",
        "val_dataset = OxfordPetDataset(val_images_filenames, images_directory, masks_directory, transform=val_transform,)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etItv_MNBY2a"
      },
      "source": [
        "## Model\n",
        "\n",
        "We will use the segmentation network from [https://github.com/milesial/Pytorch-UNet](https://github.com/milesial/Pytorch-UNet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1QZ4C7euufx"
      },
      "outputs": [],
      "source": [
        "# Convolutional block - applies two convolutional layers in a row\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "# Downscaling block - downscales image and applies convolutional block\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "# Upscaling block - upscales image and applies convolutional block\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# Final convolutional block - applies convolutional layer\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# Segmentation network\n",
        "class SegmentationNetwork(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(SegmentationNetwork, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M5_1a5TBhvb"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Start by defining the model, the optimizer, loss and metric to evaluate the model.\n",
        "\n",
        "In this case, we will use the Jaccard Index (also known as intersection over union) as the metric. Since pytorch does not have this metric implemented, we will use the ```torchmetrics``` package. [Click here](https://lightning.ai/docs/torchmetrics/stable/) to find out more about the metrics available on torchmetrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7pkokMlWSPt",
        "outputId": "54505d54-26f1-4ca0-a332-18472d8a3904"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = # TODO\n",
        "print(model)\n",
        "\n",
        "# Put model in GPU\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = # TODO\n",
        "\n",
        "# Define loss (crossentropy)\n",
        "loss_fn = # TODO\n",
        "\n",
        "# Define metric (e.g. Jaccard Index, Dice Coefficient, Pixel Accuracy)\n",
        "metric = # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7STXRuzglqk"
      },
      "source": [
        "Now implement the training cycle corresponding to one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9The7vExvoax"
      },
      "outputs": [],
      "source": [
        "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZBawSlsglql"
      },
      "source": [
        "Train the model for 10 epochs. Do not forget to use the validation set to save the best model at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWrYaHKXvvad",
        "outputId": "2b187107-30b6-408a-f022-2ee67986a99b"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLC5mHQNglql"
      },
      "source": [
        "Plot metrics and loss on training and validation sets obtained during the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtXz5ZMI2rao"
      },
      "outputs": [],
      "source": [
        "def plotTrainingHistory(train_history, val_history):\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(train_history['loss'], label='train')\n",
        "    plt.plot(val_history['loss'], label='val')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.title('Jaccard Index')\n",
        "    plt.plot(train_history['jaccard'], label='train')\n",
        "    plt.plot(val_history['jaccard'], label='val')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "TUgPQSlk2v-L",
        "outputId": "40413e90-58a8-4ea2-e63b-afbd897b505d"
      },
      "outputs": [],
      "source": [
        "plotTrainingHistory(train_history, val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Up7KWSBzLf"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnqf3r-BpubE"
      },
      "source": [
        "Visualize the results on only 10 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s3HmvIW22R-Q",
        "outputId": "eb7c1cf5-e862-4a71-f790-0ceade51297f"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "# TODO\n",
        "\n",
        "# Define test dataloader that loads only 10 images\n",
        "inference_transform = # TODO\n",
        "inference_dataset = # TODO\n",
        "inference_dataloader = # TODO\n",
        "\n",
        "# Visualize the results\n",
        "with torch.no_grad():\n",
        "    for img, mask in inference_dataloader:\n",
        "        img, mask = img.to(device), mask.to(device)\n",
        "        \n",
        "        # Get prediction\n",
        "        out = model(img)\n",
        "        probs = F.softmax(out, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "        # Show original image, mask and prediction\n",
        "        fig, ax = plt.subplots(ncols=3)\n",
        "        ax[0].imshow(img[0].cpu().numpy().transpose((1, 2, 0)))\n",
        "        ax[1].imshow(mask[0].cpu().numpy(), interpolation=\"nearest\", cmap='gray')\n",
        "        ax[2].imshow(pred[0].cpu().numpy(), interpolation=\"nearest\", cmap='gray')\n",
        "\n",
        "        ax[0].set_title(\"Image\")\n",
        "        ax[1].set_title(\"Ground truth mask\")\n",
        "        ax[2].set_title(\"Predicted mask\")\n",
        "\n",
        "        ax[0].set_axis_off()\n",
        "        ax[1].set_axis_off()\n",
        "        ax[2].set_axis_off()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "name": "python395jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "metadata": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
